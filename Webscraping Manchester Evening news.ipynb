{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1115,
   "id": "44d2ffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from bs4 import BeautifulSoup as soup\n",
    "#import requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde9d11e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file for page 271 generated successfully.\n",
      "CSV file for page 272 generated successfully.\n",
      "CSV file for page 273 generated successfully.\n",
      "CSV file for page 274 generated successfully.\n",
      "CSV file for page 275 generated successfully.\n",
      "CSV file for page 276 generated successfully.\n",
      "CSV file for page 277 generated successfully.\n",
      "CSV file for page 278 generated successfully.\n",
      "CSV file for page 279 generated successfully.\n",
      "CSV file for page 280 generated successfully.\n",
      "CSV file for page 281 generated successfully.\n",
      "CSV file for page 282 generated successfully.\n",
      "CSV file for page 283 generated successfully.\n",
      "CSV file for page 284 generated successfully.\n",
      "CSV file for page 285 generated successfully.\n",
      "CSV file for page 286 generated successfully.\n",
      "CSV file for page 287 generated successfully.\n",
      "CSV file for page 288 generated successfully.\n",
      "CSV file for page 289 generated successfully.\n",
      "CSV file for page 290 generated successfully.\n",
      "CSV file for page 291 generated successfully.\n",
      "CSV file for page 292 generated successfully.\n",
      "CSV file for page 293 generated successfully.\n",
      "CSV file for page 294 generated successfully.\n",
      "CSV file for page 295 generated successfully.\n",
      "CSV file for page 296 generated successfully.\n",
      "CSV file for page 297 generated successfully.\n",
      "CSV file for page 298 generated successfully.\n",
      "CSV file for page 299 generated successfully.\n",
      "CSV file for page 300 generated successfully.\n",
      "CSV file for page 301 generated successfully.\n",
      "CSV file for page 302 generated successfully.\n",
      "CSV file for page 303 generated successfully.\n",
      "CSV file for page 304 generated successfully.\n",
      "CSV file for page 305 generated successfully.\n",
      "CSV file for page 306 generated successfully.\n",
      "CSV file for page 307 generated successfully.\n",
      "CSV file for page 308 generated successfully.\n",
      "CSV file for page 309 generated successfully.\n",
      "CSV file for page 310 generated successfully.\n",
      "CSV file for page 311 generated successfully.\n",
      "CSV file for page 312 generated successfully.\n",
      "CSV file for page 313 generated successfully.\n",
      "CSV file for page 314 generated successfully.\n",
      "CSV file for page 315 generated successfully.\n",
      "CSV file for page 316 generated successfully.\n",
      "CSV file for page 317 generated successfully.\n",
      "CSV file for page 318 generated successfully.\n",
      "CSV file for page 319 generated successfully.\n",
      "CSV file for page 320 generated successfully.\n",
      "CSV file for page 321 generated successfully.\n",
      "CSV file for page 322 generated successfully.\n",
      "CSV file for page 323 generated successfully.\n",
      "CSV file for page 324 generated successfully.\n",
      "CSV file for page 325 generated successfully.\n",
      "CSV file for page 326 generated successfully.\n",
      "CSV file for page 327 generated successfully.\n",
      "CSV file for page 328 generated successfully.\n",
      "CSV file for page 329 generated successfully.\n",
      "CSV file for page 330 generated successfully.\n",
      "CSV file for page 331 generated successfully.\n",
      "CSV file for page 332 generated successfully.\n",
      "CSV file for page 333 generated successfully.\n",
      "CSV file for page 334 generated successfully.\n",
      "CSV file for page 335 generated successfully.\n",
      "CSV file for page 336 generated successfully.\n",
      "CSV file for page 337 generated successfully.\n",
      "CSV file for page 338 generated successfully.\n",
      "CSV file for page 339 generated successfully.\n",
      "CSV file for page 340 generated successfully.\n",
      "CSV file for page 341 generated successfully.\n",
      "CSV file for page 342 generated successfully.\n",
      "CSV file for page 343 generated successfully.\n",
      "CSV file for page 344 generated successfully.\n",
      "CSV file for page 345 generated successfully.\n",
      "CSV file for page 346 generated successfully.\n",
      "CSV file for page 347 generated successfully.\n",
      "CSV file for page 348 generated successfully.\n",
      "CSV file for page 349 generated successfully.\n",
      "CSV file for page 350 generated successfully.\n",
      "CSV file for page 351 generated successfully.\n",
      "CSV file for page 352 generated successfully.\n",
      "CSV file for page 353 generated successfully.\n",
      "CSV file for page 354 generated successfully.\n",
      "CSV file for page 355 generated successfully.\n",
      "CSV file for page 356 generated successfully.\n",
      "CSV file for page 357 generated successfully.\n",
      "CSV file for page 358 generated successfully.\n",
      "CSV file for page 359 generated successfully.\n",
      "CSV file for page 360 generated successfully.\n",
      "CSV file for page 361 generated successfully.\n",
      "CSV file for page 362 generated successfully.\n",
      "CSV file for page 363 generated successfully.\n",
      "CSV file for page 364 generated successfully.\n",
      "CSV file for page 365 generated successfully.\n",
      "CSV file for page 366 generated successfully.\n",
      "CSV file for page 367 generated successfully.\n",
      "CSV file for page 368 generated successfully.\n",
      "CSV file for page 369 generated successfully.\n",
      "CSV file for page 370 generated successfully.\n",
      "CSV file for page 371 generated successfully.\n",
      "CSV file for page 372 generated successfully.\n",
      "CSV file for page 373 generated successfully.\n",
      "CSV file for page 374 generated successfully.\n",
      "CSV file for page 375 generated successfully.\n",
      "CSV file for page 376 generated successfully.\n",
      "CSV file for page 377 generated successfully.\n",
      "CSV file for page 378 generated successfully.\n",
      "CSV file for page 379 generated successfully.\n",
      "CSV file for page 380 generated successfully.\n",
      "CSV file for page 381 generated successfully.\n",
      "CSV file for page 382 generated successfully.\n",
      "CSV file for page 383 generated successfully.\n",
      "CSV file for page 384 generated successfully.\n",
      "CSV file for page 385 generated successfully.\n",
      "CSV file for page 386 generated successfully.\n",
      "CSV file for page 387 generated successfully.\n",
      "CSV file for page 388 generated successfully.\n",
      "CSV file for page 389 generated successfully.\n",
      "CSV file for page 390 generated successfully.\n",
      "CSV file for page 391 generated successfully.\n",
      "CSV file for page 392 generated successfully.\n",
      "CSV file for page 393 generated successfully.\n",
      "CSV file for page 394 generated successfully.\n",
      "CSV file for page 395 generated successfully.\n",
      "CSV file for page 396 generated successfully.\n",
      "CSV file for page 397 generated successfully.\n",
      "CSV file for page 398 generated successfully.\n",
      "CSV file for page 399 generated successfully.\n",
      "CSV file for page 400 generated successfully.\n",
      "CSV file for page 401 generated successfully.\n",
      "CSV file for page 402 generated successfully.\n",
      "CSV file for page 403 generated successfully.\n",
      "CSV file for page 404 generated successfully.\n",
      "CSV file for page 405 generated successfully.\n",
      "CSV file for page 406 generated successfully.\n",
      "CSV file for page 407 generated successfully.\n",
      "CSV file for page 408 generated successfully.\n",
      "CSV file for page 409 generated successfully.\n",
      "CSV file for page 410 generated successfully.\n",
      "CSV file for page 411 generated successfully.\n",
      "CSV file for page 412 generated successfully.\n",
      "CSV file for page 413 generated successfully.\n",
      "CSV file for page 414 generated successfully.\n",
      "CSV file for page 415 generated successfully.\n",
      "CSV file for page 416 generated successfully.\n",
      "CSV file for page 417 generated successfully.\n",
      "CSV file for page 418 generated successfully.\n",
      "CSV file for page 419 generated successfully.\n",
      "CSV file for page 420 generated successfully.\n",
      "CSV file for page 421 generated successfully.\n",
      "CSV file for page 422 generated successfully.\n",
      "CSV file for page 423 generated successfully.\n",
      "CSV file for page 424 generated successfully.\n",
      "CSV file for page 425 generated successfully.\n",
      "CSV file for page 426 generated successfully.\n",
      "CSV file for page 427 generated successfully.\n",
      "CSV file for page 428 generated successfully.\n",
      "CSV file for page 429 generated successfully.\n",
      "CSV file for page 430 generated successfully.\n",
      "CSV file for page 431 generated successfully.\n",
      "CSV file for page 432 generated successfully.\n",
      "CSV file for page 433 generated successfully.\n",
      "CSV file for page 434 generated successfully.\n",
      "CSV file for page 435 generated successfully.\n",
      "CSV file for page 436 generated successfully.\n",
      "CSV file for page 437 generated successfully.\n",
      "CSV file for page 438 generated successfully.\n",
      "CSV file for page 439 generated successfully.\n",
      "CSV file for page 440 generated successfully.\n",
      "CSV file for page 441 generated successfully.\n",
      "CSV file for page 442 generated successfully.\n",
      "CSV file for page 443 generated successfully.\n",
      "CSV file for page 444 generated successfully.\n",
      "CSV file for page 445 generated successfully.\n",
      "CSV file for page 446 generated successfully.\n",
      "CSV file for page 447 generated successfully.\n",
      "CSV file for page 448 generated successfully.\n",
      "CSV file for page 449 generated successfully.\n",
      "CSV file for page 450 generated successfully.\n",
      "CSV file for page 451 generated successfully.\n",
      "CSV file for page 452 generated successfully.\n",
      "CSV file for page 453 generated successfully.\n",
      "CSV file for page 454 generated successfully.\n",
      "CSV file for page 455 generated successfully.\n",
      "CSV file for page 456 generated successfully.\n",
      "CSV file for page 457 generated successfully.\n",
      "CSV file for page 458 generated successfully.\n",
      "CSV file for page 459 generated successfully.\n",
      "CSV file for page 460 generated successfully.\n",
      "CSV file for page 461 generated successfully.\n",
      "CSV file for page 462 generated successfully.\n",
      "CSV file for page 463 generated successfully.\n",
      "CSV file for page 464 generated successfully.\n",
      "CSV file for page 465 generated successfully.\n",
      "CSV file for page 466 generated successfully.\n",
      "CSV file for page 467 generated successfully.\n",
      "CSV file for page 468 generated successfully.\n",
      "CSV file for page 469 generated successfully.\n",
      "CSV file for page 470 generated successfully.\n",
      "CSV file for page 471 generated successfully.\n",
      "CSV file for page 472 generated successfully.\n",
      "CSV file for page 473 generated successfully.\n",
      "CSV file for page 474 generated successfully.\n",
      "CSV file for page 475 generated successfully.\n",
      "CSV file for page 476 generated successfully.\n",
      "CSV file for page 477 generated successfully.\n",
      "CSV file for page 478 generated successfully.\n",
      "CSV file for page 479 generated successfully.\n",
      "CSV file for page 480 generated successfully.\n",
      "CSV file for page 481 generated successfully.\n",
      "CSV file for page 482 generated successfully.\n",
      "CSV file for page 483 generated successfully.\n",
      "CSV file for page 484 generated successfully.\n",
      "CSV file for page 485 generated successfully.\n",
      "CSV file for page 486 generated successfully.\n",
      "CSV file for page 487 generated successfully.\n",
      "CSV file for page 488 generated successfully.\n",
      "CSV file for page 489 generated successfully.\n",
      "CSV file for page 490 generated successfully.\n",
      "CSV file for page 491 generated successfully.\n",
      "CSV file for page 492 generated successfully.\n",
      "CSV file for page 493 generated successfully.\n",
      "CSV file for page 494 generated successfully.\n",
      "CSV file for page 495 generated successfully.\n",
      "CSV file for page 496 generated successfully.\n",
      "CSV file for page 497 generated successfully.\n",
      "CSV file for page 498 generated successfully.\n",
      "CSV file for page 499 generated successfully.\n",
      "CSV file for page 500 generated successfully.\n",
      "CSV file for page 501 generated successfully.\n",
      "CSV file for page 502 generated successfully.\n",
      "CSV file for page 503 generated successfully.\n",
      "CSV file for page 504 generated successfully.\n",
      "CSV file for page 505 generated successfully.\n",
      "CSV file for page 506 generated successfully.\n",
      "CSV file for page 507 generated successfully.\n",
      "CSV file for page 508 generated successfully.\n",
      "CSV file for page 509 generated successfully.\n",
      "CSV file for page 510 generated successfully.\n",
      "CSV file for page 511 generated successfully.\n",
      "CSV file for page 512 generated successfully.\n",
      "CSV file for page 513 generated successfully.\n",
      "CSV file for page 514 generated successfully.\n",
      "CSV file for page 515 generated successfully.\n",
      "CSV file for page 516 generated successfully.\n",
      "CSV file for page 517 generated successfully.\n",
      "CSV file for page 518 generated successfully.\n",
      "CSV file for page 519 generated successfully.\n",
      "CSV file for page 520 generated successfully.\n",
      "CSV file for page 521 generated successfully.\n",
      "CSV file for page 522 generated successfully.\n",
      "CSV file for page 523 generated successfully.\n",
      "CSV file for page 524 generated successfully.\n",
      "CSV file for page 525 generated successfully.\n",
      "CSV file for page 526 generated successfully.\n",
      "CSV file for page 527 generated successfully.\n",
      "CSV file for page 528 generated successfully.\n",
      "CSV file for page 529 generated successfully.\n",
      "CSV file for page 530 generated successfully.\n",
      "CSV file for page 531 generated successfully.\n",
      "CSV file for page 532 generated successfully.\n",
      "CSV file for page 533 generated successfully.\n",
      "CSV file for page 534 generated successfully.\n",
      "CSV file for page 535 generated successfully.\n",
      "CSV file for page 536 generated successfully.\n",
      "CSV file for page 537 generated successfully.\n",
      "CSV file for page 538 generated successfully.\n",
      "CSV file for page 539 generated successfully.\n",
      "CSV file for page 540 generated successfully.\n",
      "CSV file for page 541 generated successfully.\n",
      "CSV file for page 542 generated successfully.\n",
      "CSV file for page 543 generated successfully.\n",
      "CSV file for page 544 generated successfully.\n",
      "CSV file for page 545 generated successfully.\n",
      "CSV file for page 546 generated successfully.\n",
      "CSV file for page 547 generated successfully.\n",
      "CSV file for page 548 generated successfully.\n",
      "CSV file for page 549 generated successfully.\n",
      "CSV file for page 550 generated successfully.\n",
      "CSV file for page 551 generated successfully.\n",
      "CSV file for page 552 generated successfully.\n",
      "CSV file for page 553 generated successfully.\n",
      "CSV file for page 554 generated successfully.\n",
      "CSV file for page 555 generated successfully.\n",
      "CSV file for page 556 generated successfully.\n",
      "CSV file for page 557 generated successfully.\n",
      "CSV file for page 558 generated successfully.\n",
      "CSV file for page 559 generated successfully.\n",
      "CSV file for page 560 generated successfully.\n",
      "CSV file for page 561 generated successfully.\n",
      "CSV file for page 562 generated successfully.\n",
      "CSV file for page 563 generated successfully.\n",
      "CSV file for page 564 generated successfully.\n",
      "CSV file for page 565 generated successfully.\n",
      "CSV file for page 566 generated successfully.\n",
      "CSV file for page 567 generated successfully.\n",
      "CSV file for page 568 generated successfully.\n",
      "CSV file for page 569 generated successfully.\n",
      "CSV file for page 570 generated successfully.\n",
      "CSV file for page 571 generated successfully.\n",
      "CSV file for page 572 generated successfully.\n",
      "CSV file for page 573 generated successfully.\n",
      "CSV file for page 574 generated successfully.\n",
      "CSV file for page 575 generated successfully.\n",
      "CSV file for page 576 generated successfully.\n",
      "CSV file for page 577 generated successfully.\n",
      "CSV file for page 578 generated successfully.\n",
      "CSV file for page 579 generated successfully.\n",
      "CSV file for page 580 generated successfully.\n",
      "CSV file for page 581 generated successfully.\n",
      "CSV file for page 582 generated successfully.\n",
      "CSV file for page 583 generated successfully.\n",
      "CSV file for page 584 generated successfully.\n",
      "CSV file for page 585 generated successfully.\n",
      "CSV file for page 586 generated successfully.\n",
      "CSV file for page 587 generated successfully.\n",
      "CSV file for page 588 generated successfully.\n",
      "CSV file for page 589 generated successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import csv\n",
    "\n",
    "for page_number in range(271, 2000):\n",
    "    data = []\n",
    "    # Check if the number is less than 526\n",
    "    if page_number < 1001:\n",
    "        manchester_living_url = f'https://www.manchestereveningnews.co.uk/all-about/salford?pageNumber={page_number}'\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "        r = requests.get(manchester_living_url, headers=headers)\n",
    "        Soup = soup(r.content, 'lxml')\n",
    "        \n",
    "        # Generate links in an array\n",
    "        links = []\n",
    "        for news in Soup.findAll('div', {'class': 'teaser'}):\n",
    "            links.append(news.a['href'])\n",
    "            \n",
    "        for link in links: \n",
    "            page = requests.get(link)\n",
    "            bsoup_obj = soup(page.content)\n",
    "            for article in bsoup_obj.findAll('article'):\n",
    "                # Instantiate an object of the article\n",
    "                structured_article = {}\n",
    "\n",
    "                # Spread out items to object names\n",
    "                author_element = article.find('a', class_='publication-theme')\n",
    "                if author_element:\n",
    "                    structured_article['Author'] = author_element.text.strip()\n",
    "                else:\n",
    "                    structured_article['Author'] = \"\"\n",
    "\n",
    "                author_title_element = article.find('span', class_='job-title')\n",
    "                if author_title_element:\n",
    "                    structured_article['Author Title'] = author_title_element.text.strip()\n",
    "                else:\n",
    "                    structured_article['Author Title'] = \"\"\n",
    "\n",
    "                headline_element = article.find('h1')\n",
    "                if headline_element:\n",
    "                    structured_article['Headline'] = headline_element.text.strip()\n",
    "                else:\n",
    "                    structured_article['Headline'] = \"\"\n",
    "\n",
    "                sector_element = bsoup_obj.find('ol')\n",
    "                if sector_element:\n",
    "                    sector_texts = []\n",
    "\n",
    "                    first_child_skipped = False\n",
    "                    for child in sector_element.children:\n",
    "                        if not first_child_skipped:\n",
    "                            first_child_skipped = True\n",
    "                            continue\n",
    "\n",
    "                        text = child.text.strip()\n",
    "\n",
    "                        if text:\n",
    "                            sector_texts.append(text)\n",
    "\n",
    "                    sector_text_combined = '/'.join(sector_texts)\n",
    "\n",
    "                    structured_article['Sector'] = sector_text_combined\n",
    "                else:\n",
    "                    structured_article['Sector'] = ''\n",
    "\n",
    "                article_element = article.find('div', class_='article-body')\n",
    "                if article_element:\n",
    "                    structured_article['article'] = article_element.text.strip()\n",
    "                else:\n",
    "                    structured_article['article'] = \"\"\n",
    "\n",
    "                date_element = article.find('span', class_='time-container')\n",
    "                if date_element:\n",
    "                    structured_article['date'] = date_element.text.strip().split(', ')[1]\n",
    "                else:\n",
    "                    structured_article['date'] = \"\"\n",
    "\n",
    "                structured_article['Site URL'] = link\n",
    "\n",
    "                data.append(structured_article)\n",
    "\n",
    "    else:\n",
    "        # If the number is equal to or greater than 526, stop the loop\n",
    "        break\n",
    "\n",
    "    # Define headers for the article\n",
    "    headers = ['Author', 'Author Title', 'Sector', 'Headline', 'article', 'date', 'Site URL']\n",
    "\n",
    "    # Write the data to a CSV file\n",
    "    with open(f'Salford_{page_number}.csv', 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "    print(f\"CSV file for page {page_number} generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fae85d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#previous implementation\n",
    "manchester_living_url = 'https://www.lancashiretelegraph.co.uk/author/profile/71324.Melanie_Disley/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1117,
   "id": "4de6ede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Querying site by author, using the API endpoint. \n",
    "\n",
    "# import requests\n",
    "#import json\n",
    "\n",
    "# URL of the JSON endpoint\n",
    "#url = 'https://eu-gb.functions.appdomain.cloud/api/v1/web/b8a06c2a-7878-4a1a-a01a-7fa11888a6e7/news_assistant_package/search-articles-api-2?search_text_all=&search_text=&search_text_none=&mantis_categories=&tags=&domains=manchestereveningnews&excluded_domains=&author=Tyrone%20Marshall'\n",
    "# Fetch the JSON data\n",
    "#response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "#if response.status_code == 200:\n",
    "    # Parse the JSON data\n",
    " #   data = json.loads(response.text)\n",
    "    \n",
    "    # Access the json data and extract the URL:\n",
    " #   links = []\n",
    " #   for person in data['articleData']:\n",
    " #       new_url = 'https://'+ person['url']\n",
    " #       links.append(new_url)\n",
    "#else:\n",
    "#    print('Failed to retrieve data:', response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1118,
   "id": "6ee980c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "41ba49e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## previous implementation\n",
    "r = requests.get(manchester_living_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1edd5e03-b24a-45f4-bef1-094ff2c7d2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "37de4ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "## previous implementation\n",
    "Soup = soup(r.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6b21f647-ecf9-4af7-a2d3-45c989a8294c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.lancashiretelegraph.co.uk/news/24271036.113-million-grabs-tuesdays-euromillions-jackpot/',\n",
       " 'https://www.lancashiretelegraph.co.uk/news/24268189.mark-menzies-quits-conservatives-tories-say-conduct-standard/',\n",
       " 'https://www.lancashiretelegraph.co.uk/news/24266611.wifes-drowning-pool-suspicious-friend-tells-court/',\n",
       " 'https://www.lancashiretelegraph.co.uk/news/24266475.ramsbottom-owens-restaurant-bar-worth-visit-brunch/',\n",
       " 'https://www.lancashiretelegraph.co.uk/news/24266451.police-reviewing-allegations-mp-mark-menzies-misused-campaign-funds/',\n",
       " 'https://www.lancashiretelegraph.co.uk/news/24256576.kirkham-prison-librarian-not-guilty-sex-assaults-inmates/',\n",
       " 'https://www.lancashiretelegraph.co.uk/news/24244704.hoghton-retro-1970s-holiday-bungalow-loved-visitors/']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a5db52e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lottery operators Allwyn said the EuroMillions jackpot would be the second won in the UK so far this year, after Richard and Debbie Nuttall scooped the Â£61 million prize in January.\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## previous implementation\n",
    "links = []\n",
    "for news in Soup.findAll('div', {'class' : 'mar-small-card__img-icon-container'}):\n",
    "    links.append(\"https://www.lancashiretelegraph.co.uk\" + news.a['href'])\n",
    "    \n",
    "links\n",
    "for link in links:\n",
    "    article_text = \"\"\n",
    "    article_tag = Soup.find('article')\n",
    "    if article_tag:\n",
    "        paragraphs = article_tag.findAll('p')\n",
    "        for paragraph in paragraphs:\n",
    "            article_text += paragraph.text.strip() + '\\n'\n",
    "article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5b338c13-4aa6-4418-ac02-4c0a77782569",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Extract article text\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Parse HTML content\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[1;32m----> 4\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m links:\n\u001b[0;32m      6\u001b[0m     article_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract article text\n",
    "# Parse HTML content\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "for link in links:\n",
    "    article_text = \"\"\n",
    "    article_tag = soup.find('article')\n",
    "    if article_tag:\n",
    "        paragraphs = article_tag.findAll('p')\n",
    "        for paragraph in paragraphs:\n",
    "            article_text += paragraph.text.strip() + '\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a4ed0a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "## previous implementation\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1123,
   "id": "bc8004d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1124,
   "id": "84298abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file generated successfully.\n"
     ]
    }
   ],
   "source": [
    "#import requests\n",
    "#from bs4 import BeautifulSoup as soup\n",
    "#import csv\n",
    "\n",
    "data = []\n",
    "\n",
    "for link in links: \n",
    "    page = requests.get(link)\n",
    "    bsoup_obj = soup(page.content)\n",
    "    for article in bsoup_obj.findAll('article'):\n",
    "        # Instantiate an object of the article\n",
    "        structured_article = {}\n",
    "\n",
    "        # Spread out items to object names\n",
    "        \n",
    "        ### Find the span with class 'author'\n",
    "        author_element = article.find('a', class_='publication-theme')\n",
    "        # Check if the element exists\n",
    "        if author_element:\n",
    "            # If the element exists, assign its text content to 'Structured_title['Author Title']'\n",
    "            structured_article['Author'] = author_element.text.strip()\n",
    "        else:\n",
    "            # If the element doesn't exist, assign an empty string to 'Structured_title['Author Title']'\n",
    "            structured_article['Author'] = \"\"\n",
    "\n",
    "        ### Find the span with class 'job-title'\n",
    "        author_title_element = article.find('span', class_='job-title')\n",
    "        # Check if the element exists\n",
    "        if author_title_element:\n",
    "            # If the element exists, assign its text content to 'Structured_title['Author Title']'\n",
    "            structured_article['Author Title'] = author_title_element.text.strip()\n",
    "        else:\n",
    "            # If the element doesn't exist, assign an empty string to 'Structured_title['Author Title']'\n",
    "            structured_article['Author Title'] = \"\"\n",
    "\n",
    "        ##### Find the span with class 'headline'\n",
    "        headline_element = article.find('h1')\n",
    "        # Check if the element exists\n",
    "        if headline_element:\n",
    "            # If the element exists, assign its text content to 'Structured_title['Author Title']'\n",
    "            structured_article['Headline'] = headline_element.text.strip()\n",
    "        else:\n",
    "            # If the element doesn't exist, assign an empty string to 'Structured_title['Author Title']'\n",
    "            structured_article['Headline'] = \"\"\n",
    "\n",
    "        ##### Find the span with class 'sector'\n",
    "        sector_element = bsoup_obj.find('ol')\n",
    "        # Check if the element exists\n",
    "        if sector_element:\n",
    "            # Initialize an empty list to store the texts of children\n",
    "            sector_texts = []\n",
    "\n",
    "            # Iterate over the children of the <nav> element, skipping the first child\n",
    "            first_child_skipped = False\n",
    "            for child in sector_element.children:\n",
    "                # Skip the first child\n",
    "                if not first_child_skipped:\n",
    "                    first_child_skipped = True\n",
    "                    continue\n",
    "                \n",
    "                # Extract the text of the child and strip any leading or trailing whitespaces\n",
    "                text = child.text.strip()\n",
    "                \n",
    "                # Check if the extracted text is not empty\n",
    "                if text:\n",
    "                    # Add the non-empty text to the list\n",
    "                    sector_texts.append(text)\n",
    "            \n",
    "            # Combine the texts of children into a single string with space separator\n",
    "            sector_text_combined = '/'.join(sector_texts)\n",
    "\n",
    "            # Assign the combined text to 'structured_article['Sector']'\n",
    "            structured_article['Sector'] = sector_text_combined\n",
    "        else:\n",
    "            # If the <nav> element doesn't exist, assign an empty string to 'structured_article['Sector']'\n",
    "            structured_article['Sector'] = ''\n",
    "\n",
    "        ##### Find the span with class 'article'\n",
    "        article_element = article.find('div', class_='article-body')\n",
    "        # Check if the element exists\n",
    "        if article_element:\n",
    "            # If the element exists, assign its text content to 'Structured_title['Author Title']'\n",
    "            structured_article['article'] = article_element.text.strip()\n",
    "        else:\n",
    "            # If the element doesn't exist, assign an empty string to 'Structured_title['Author Title']'\n",
    "            structured_article['article'] = \"\"\n",
    "\n",
    "        ##### Find the span with class 'date'\n",
    "        date_element = article.find('span', class_='time-container')\n",
    "        # Check if the element exists\n",
    "        if date_element:\n",
    "            # If the element exists, assign its text content to 'Structured_title['Author Title']'\n",
    "            structured_article['date'] = date_element.text.strip().split(', ')[1]\n",
    "        else:\n",
    "            # If the element doesn't exist, assign an empty string to 'Structured_title['Author Title']'\n",
    "            structured_article['date'] = \"\"\n",
    "\n",
    "        # structured_article['Author'] = article.find('a', class_='publication-theme').text.strip()\n",
    "        # structured_article['Author Title'] = article.find('span', class_='job-title').text.strip()\n",
    "        # structured_article['Headline'] = article.find('h1').text.strip()\n",
    "        # structured_article['article'] = article.find('div', class_='article-body').text.strip()\n",
    "        # structured_article['date'] = article.find('span', class_='time-container').text.strip().split(', ')[1]\n",
    "\n",
    "        structured_article['Site URL'] = link\n",
    "\n",
    "        data.append(structured_article)\n",
    "\n",
    "# Define headers for the article\n",
    "headers = ['Author', 'Author Title', 'Sector', 'Headline', 'article', 'date', 'Site URL']\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open('Bolton_47.csv', 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(\"CSV file generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1132,
   "id": "8e8cf23d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Set the directory containing CSV files\n",
    "directory = '//DESKTOP-9U2A6M9/Users/Ayoola/Downloads/New%20Downloads/OneDrive_1_3-1-2024'\n",
    "\n",
    "# Initialize an empty list to store the file paths\n",
    "csv_files = []\n",
    "\n",
    "# Recursively find all CSV files in the directory and its subdirectories\n",
    "for filename in glob.iglob(os.path.join(directory, '**/*.csv'), recursive=True):\n",
    "    csv_files.append(filename)\n",
    "\n",
    "# Combine contents of all CSV files into one file\n",
    "with open('merged_.csv', 'w', newline='', encoding='utf-8') as merged_file:\n",
    "    for csv_file in csv_files:\n",
    "        with open(csv_file, 'r', encoding='utf-8') as file:\n",
    "            merged_file.write(file.read())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
