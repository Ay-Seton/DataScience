{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2ec264e-7690-49d2-9076-61dea1b61adf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# IMPLEMENTATION FOR BLACKPOOL GAZZETTER NEWSPAPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bef8500e-e626-47bd-a750-d2ce3df4d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_article_texts(author_url, output_file):\n",
    "    # Fetch HTML content\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.4'}\n",
    "    response = requests.get(author_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to fetch the webpage\")\n",
    "        return\n",
    "\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract links\n",
    "    links = []\n",
    "    for news in soup.find_all('div', class_='sc-jmnVvD eASYYc article-item sc-cUEIKg cpfUge'):\n",
    "        links.append(\"https://www.blackpoolgazette.co.uk\" + news.a['href'])\n",
    "\n",
    "    # Scrape article text from each link\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['URL', 'Article Text'])\n",
    "\n",
    "        for link in links:\n",
    "            # Fetch HTML content of the article page\n",
    "            response = requests.get(link, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch the webpage: {link}\")\n",
    "                continue\n",
    "\n",
    "            # Parse HTML content\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Extract article text\n",
    "            article_text = \"\"\n",
    "            article_tag = soup.find('article')\n",
    "            if article_tag:\n",
    "                paragraphs = article_tag.find_all('p')\n",
    "                for paragraph in paragraphs:\n",
    "                    article_text += paragraph.text.strip() + '\\n'\n",
    "            else:\n",
    "                article_div = soup.find('div', class_=lambda x: x and 'article' in x.lower())\n",
    "                if article_div:\n",
    "                    paragraphs = article_div.find_all('p')\n",
    "                    for paragraph in paragraphs:\n",
    "                        article_text += paragraph.text.strip() + '\\n'\n",
    "                else:\n",
    "                    print(\"Failed to find article text for URL:\", link)\n",
    "                    continue\n",
    "\n",
    "            # Write URL and article text to CSV file\n",
    "            writer.writerow([link, article_text])\n",
    "\n",
    "    print(\"Article texts extracted and saved to:\", output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "641c89a6-1510-4d72-9734-07b147c0415a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article texts extracted and saved to: Vanessa_sims.csv\n"
     ]
    }
   ],
   "source": [
    "# usage:\n",
    "Blackpool_url = 'https://www.blackpoolgazette.co.uk/author/vanessa-sims'\n",
    "output_file = 'Vanessa_sims.csv'\n",
    "scrape_article_texts(Blackpool_url, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb808873-d6fa-41f0-acd9-65ce2e680774",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# IMPLEMENTATION FOR LANCASHIRE TIMES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "89cd6bf6-4ecc-4e4f-b01c-401a8a11a45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_article_texts(author_url, output_file):\n",
    "    # Fetch HTML content\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.4'}\n",
    "    response = requests.get(author_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to fetch the webpage\")\n",
    "        return\n",
    "\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract links\n",
    "    links = []\n",
    "    for news in soup.find_all('div', class_='gtcol'):   ### Wrapper for the links to the news article\n",
    "        links.append(\"https://lancashiretimes.co.uk/\" + news.a['href'])\n",
    "\n",
    "    # Scrape article text from each link\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['URL', 'Article Text'])\n",
    "\n",
    "        for link in links:\n",
    "            # Fetch HTML content of the article page\n",
    "            response = requests.get(link, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch the webpage: {link}\")\n",
    "                continue\n",
    "\n",
    "            # Parse HTML content\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Extract article text\n",
    "            article_text = \"\"\n",
    "            article_tag = soup.find('div', class_='article')\n",
    "            if article_tag:\n",
    "                    # Find all <panel_100> tags and remove their text content from the article\n",
    "                   for panel_100 in article_tag.find_all('panel_100'):\n",
    "                         panel_100.extract()\n",
    "\n",
    "                   article_text += article_tag.text.strip() + '\\n'\n",
    "            else:\n",
    "                article_div = soup.find('div', class_=lambda x: x and 'article' in x.lower())\n",
    "                if article_div:\n",
    "                    paragraphs = article_div.find_all('p')\n",
    "                    for paragraph in paragraphs:\n",
    "                        article_text += paragraph.text.strip() + '\\n'\n",
    "                else:\n",
    "                    print(\"Failed to find article text for URL:\", link)\n",
    "                    continue\n",
    "\n",
    "            # Write URL and article text to CSV file\n",
    "            writer.writerow([link, article_text])\n",
    "\n",
    "    print(\"Article texts extracted and saved to:\", output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0d9415ef-5233-4788-ab1f-dbc6970b41fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article texts extracted and saved to: grahamclark.csv\n"
     ]
    }
   ],
   "source": [
    "lancashire_url = 'https://lancashiretimes.co.uk/writer/grahamclark'\n",
    "output_file = 'grahamclark.csv'\n",
    "scrape_article_texts(lancashire_url, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebc098d-99a1-4048-acbc-eb826743be8a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# IMPLEMENTATION FOR LANCASHIRE TELEGRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc2752aa-c250-4856-9c4e-02ec136429d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_article_texts(author_url, output_file):\n",
    "    # Fetch HTML content\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.4'}\n",
    "    response = requests.get(author_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to fetch the webpage\")\n",
    "        return\n",
    "\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    print(\"soup\")\n",
    "    # Extract links\n",
    "    links = []\n",
    "    for news in soup.find_all('div', class_='mar-small-card__img-icon-container'):\n",
    "        links.append(\"https://www.lancashiretelegraph.co.uk\" + news.a['href'])\n",
    "        print(\"loop\")\n",
    "        \n",
    "    # Scrape article text from each link\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['URL', 'Article Text'])\n",
    "\n",
    "        for link in links:\n",
    "            # Fetch HTML content of the article page\n",
    "            response = requests.get(link, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch the webpage: {link}\")\n",
    "                continue\n",
    "\n",
    "            # Parse HTML content\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            #print(soup)\n",
    "            # Extract article text\n",
    "            article_text = \"\"\n",
    "            paragraphs = soup.find_all('p')\n",
    "            #print(paragraphs)\n",
    "            if paragraphs: \n",
    "                for paragraph in paragraphs:\n",
    "                    article_text += paragraph.text.strip() + '\\n'\n",
    "                    #print(article_text)\n",
    "           \n",
    "            else:\n",
    "                article_div = soup.find('div', class_=lambda x: x and 'article' in x.lower())\n",
    "                if article_div:\n",
    "                    paragraphs = article_div.find_all('p')\n",
    "                    for paragraph in paragraphs:\n",
    "                        article_text += paragraph.text.strip() + '\\n'\n",
    "                else:\n",
    "                    print(\"Failed to find article text for URL:\", link)\n",
    "                    continue\n",
    "\n",
    "            # Write URL and article text to CSV file\n",
    "            writer.writerow([link, article_text])\n",
    "\n",
    "    print(\"Article texts extracted and saved to:\", output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b4bed4b-84db-4a1f-9d8f-cc71378eac0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soup\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "Article texts extracted and saved to: Emma_mayoh.csv\n"
     ]
    }
   ],
   "source": [
    "lancashiretelegraph_url = 'https://www.lancashiretelegraph.co.uk/author/profile/314878.Emma_Mayoh/'\n",
    "output_file = 'Emma_mayoh.csv'\n",
    "scrape_article_texts(lancashiretelegraph_url, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72965fdc-c1e7-4240-afea-ae977b3797f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# IMPLEMENTATION FOR THE SKINNY UK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe574211-ba43-476e-9b8e-fce487d42ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_article_texts(author_url, output_file):\n",
    "    # Fetch HTML content\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.4'}\n",
    "    response = requests.get(author_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to fetch the webpage\")\n",
    "        return\n",
    "\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    print(\"soup\")\n",
    "    # Extract links\n",
    "    links = []\n",
    "    news = soup.find('ul', class_='record-list list-view default-list')\n",
    "    for new in news.find_all('a'): \n",
    "        links.append(\"https://www.theskinny.co.uk/\" + new['href'])\n",
    "        print(\"loop\")\n",
    "        \n",
    "    # Scrape article text from each link\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['URL', 'Article Text'])\n",
    "\n",
    "        for link in links:\n",
    "            # Fetch HTML content of the article page\n",
    "            response = requests.get(link, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch the webpage: {link}\")\n",
    "                continue\n",
    "\n",
    "            # Parse HTML content\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            #print(soup)\n",
    "            # Extract article text\n",
    "            article_text = \"\"\n",
    "            paragraphs = soup.find_all('p')\n",
    "            #print(paragraphs)\n",
    "            if paragraphs: \n",
    "                for paragraph in paragraphs:\n",
    "                    article_text += paragraph.text.strip() + '\\n'\n",
    "                    #print(article_text)\n",
    "           \n",
    "            else:\n",
    "                article_div = soup.find('div', class_=lambda x: x and 'article' in x.lower())\n",
    "                if article_div:\n",
    "                    paragraphs = article_div.find_all('p')\n",
    "                    for paragraph in paragraphs:\n",
    "                        article_text += paragraph.text.strip() + '\\n'\n",
    "                else:\n",
    "                    print(\"Failed to find article text for URL:\", link)\n",
    "                    continue\n",
    "\n",
    "            # Write URL and article text to CSV file\n",
    "            writer.writerow([link, article_text])\n",
    "\n",
    "    print(\"Article texts extracted and saved to:\", output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2cd2fd-2acb-4a86-b8e8-4b8ec9162693",
   "metadata": {},
   "outputs": [],
   "source": [
    "skinny_uk_url = 'https://www.theskinny.co.uk/search?utf8=%E2%9C%93&q=Tallah+Brash'\n",
    "output_file = 'Tallah_Brash.csv'\n",
    "scrape_article_texts(skinny_uk_url, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726091ac-0d7a-4993-ac3a-5c8d4b7794cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## IMPLEMENTATION FOR CHRIS HORKAN ---- NO AUTHOR SEARCH ON THE WEBSITE // using muckrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5858de1b-60f3-4211-bf0a-e1ddc73feb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_article_texts(author_url, output_file):\n",
    "    # Fetch HTML content\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    response = requests.get(author_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to fetch the webpage\")\n",
    "        return\n",
    "\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    print(\"soup\")\n",
    "    # Extract links\n",
    "    links = []\n",
    "    news = soup.find_all('h4', class_='news-story-title')\n",
    "    for new in news: \n",
    "        links.append(new.a['href'])\n",
    "        print(\"loop\")\n",
    "        \n",
    "    # Scrape article text from each link\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['URL', 'Article Text'])\n",
    "\n",
    "        for link in links:\n",
    "            # Fetch HTML content of the article page\n",
    "            response = requests.get(link, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch the webpage: {link}\")\n",
    "                continue\n",
    "\n",
    "            # Parse HTML content\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            #print(soup)\n",
    "            # Extract article text\n",
    "            article_text = \"\"\n",
    "            paragraphs = soup.find_all('p')\n",
    "            #print(paragraphs)\n",
    "            if paragraphs: \n",
    "                for paragraph in paragraphs:\n",
    "                    article_text += paragraph.text.strip() + '\\n'\n",
    "                    #print(article_text)\n",
    "           \n",
    "            else:\n",
    "                article_div = soup.find('div', class_=lambda x: x and 'article' in x.lower())\n",
    "                if article_div:\n",
    "                    paragraphs = article_div.find_all('p')\n",
    "                    for paragraph in paragraphs:\n",
    "                        article_text += paragraph.text.strip() + '\\n'\n",
    "                else:\n",
    "                    print(\"Failed to find article text for URL:\", link)\n",
    "                    continue\n",
    "\n",
    "            # Write URL and article text to CSV file\n",
    "            writer.writerow([link, article_text])\n",
    "\n",
    "    print(\"Article texts extracted and saved to:\", output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00153f4f-8c5a-4f81-8eb1-abd1a5fb19b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soup\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "Failed to fetch the webpage: https://www.creativetourist.com/cumbria/music-venues-cumbria/\n",
      "Failed to fetch the webpage: https://www.creativetourist.com/sheffield/music-venues-sheffield/\n",
      "Failed to fetch the webpage: https://www.creativetourist.com/liverpool/music-venues-liverpool/\n",
      "Failed to fetch the webpage: https://www.creativetourist.com/event/rncm-summer-season-2018/\n",
      "Article texts extracted and saved to: Chris_Horkan.csv\n"
     ]
    }
   ],
   "source": [
    "Creative_url = 'https://muckrack.com/chris-horkan/articles'\n",
    "output_file = 'Chris_Horkan.csv'\n",
    "scrape_article_texts(Creative_url, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a4baf8-6a92-40c5-b13d-b560ec440d8c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# MANUAL IMPLEMENTATION FOR EMAH MAYO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a718bbc3-c737-4bdf-b167-792b288fc945",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://yorkshirebylines.co.uk/society/culture/a-winner-all-the-way-the-syndicate-at-the-leeds-grand-theatre/'\n",
    "link = url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f378020-f4c7-4b7c-b04b-3ea875c68d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.4'}\n",
    "response = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d4c4e32-4999-4c90-937a-2162671b2491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c01fe33c-421c-4738-a1e4-4495a63e45af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract article text\n",
    "article_text = \"\"\n",
    "paragraphs = soup.find_all('p')\n",
    " #print(paragraphs)\n",
    "if paragraphs: \n",
    "   for paragraph in paragraphs:\n",
    "       article_text += paragraph.text.strip() + '\\n'\n",
    "        #print(article_text)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f74dac-62c3-420e-b9e0-cdf92c19d6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d272aef3-05c8-4a0d-a3ab-886a85ccad47",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Scrape article text from each link\n",
    "with open(\"graham_clark.csv\", 'w', newline='', encoding='utf-8') as csv_file:\n",
    "     writer = csv.writer(csv_file)\n",
    "     writer.writerow(['URL', 'Article Text'])\n",
    "         # Write URL and article text to CSV file\n",
    "     writer.writerow([link, article_text])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a47bddf-d36e-46bc-9f81-d96cc82e4cff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# IMPLEMENTATION FOR CATHERINE MUGONYI USING MUCKRACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2486d4be-ea59-44bc-ace2-d1c4cc492337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_article_texts(author_url, output_file):\n",
    "    # Fetch HTML content\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.4'}\n",
    "    response = requests.get(author_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to fetch the webpage\")\n",
    "        return\n",
    "\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    print(\"soup\")\n",
    "    # Extract links\n",
    "    links = []\n",
    "    news = soup.find_all('h4', class_='news-story-title')\n",
    "    for new in news: \n",
    "        links.append(new.a['href'])\n",
    "        print(\"loop\")\n",
    "        \n",
    "    # Scrape article text from each link\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['URL', 'Article Text'])\n",
    "\n",
    "        for link in links:\n",
    "            # Fetch HTML content of the article page\n",
    "            response = requests.get(link, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch the webpage: {link}\")\n",
    "                continue\n",
    "\n",
    "            # Parse HTML content\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            #print(soup)\n",
    "            # Extract article text\n",
    "            article_text = \"\"\n",
    "            paragraphs = soup.find_all('p')\n",
    "            #print(paragraphs)\n",
    "            if paragraphs: \n",
    "                for paragraph in paragraphs:\n",
    "                    article_text += paragraph.text.strip() + '\\n'\n",
    "                    #print(article_text)\n",
    "           \n",
    "            else:\n",
    "                article_div = soup.find('div', class_=lambda x: x and 'article' in x.lower())\n",
    "                if article_div:\n",
    "                    paragraphs = article_div.find_all('p')\n",
    "                    for paragraph in paragraphs:\n",
    "                        article_text += paragraph.text.strip() + '\\n'\n",
    "                else:\n",
    "                    print(\"Failed to find article text for URL:\", link)\n",
    "                    continue\n",
    "\n",
    "            # Write URL and article text to CSV file\n",
    "            writer.writerow([link, article_text])\n",
    "\n",
    "    print(\"Article texts extracted and saved to:\", output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d483ea7-3123-4c67-806c-d7556316e026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soup\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "Failed to find article text for URL: http://www.altblackpool.co.uk/27588-family-fun-at-the-big-pier-watch-day/\n",
      "Failed to find article text for URL: http://www.altblackpool.co.uk/27364-americanidiot/\n",
      "Failed to find article text for URL: http://www.altblackpool.co.uk/27436-blanket-reveal-debut-album/\n",
      "Failed to find article text for URL: http://www.altblackpool.co.uk/26695-oxide-ghosts/\n",
      "Failed to find article text for URL: http://www.altblackpool.co.uk/26640-town-centre-christmas/\n",
      "Failed to find article text for URL: http://www.altblackpool.co.uk/26195-stars-wanted-for-lytham-tv/\n",
      "Failed to find article text for URL: http://www.altblackpool.co.uk/26136-retroscope/\n",
      "Article texts extracted and saved to: Catherine_Mugonyi.csv\n"
     ]
    }
   ],
   "source": [
    "Creative_url = 'https://muckrack.com/catherine-mugonyi/articles'\n",
    "output_file = 'Catherine_Mugonyi.csv'\n",
    "scrape_article_texts(Creative_url, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315bc166-8a27-4609-86cd-e96f2a85ee0c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# IMPLEMENTATION FOR ABBIE JONES THROUGH MUCKRACK.COM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2627445-e8ee-492f-83b6-c6faf2992998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_article_texts(author_url, output_file):\n",
    "    # Fetch HTML content\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.4'}\n",
    "    response = requests.get(author_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to fetch the webpage\")\n",
    "        return\n",
    "\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    print(\"soup\")\n",
    "    # Extract links\n",
    "    links = []\n",
    "    news = soup.find_all('h4', class_='news-story-title')\n",
    "    for new in news: \n",
    "        links.append(new.a['href'])\n",
    "        print(\"loop\")\n",
    "        \n",
    "    # Scrape article text from each link\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['URL', 'Article Text'])\n",
    "\n",
    "        for link in links:\n",
    "            # Fetch HTML content of the article page\n",
    "            response = requests.get(link, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch the webpage: {link}\")\n",
    "                continue\n",
    "\n",
    "            # Parse HTML content\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            #print(soup)\n",
    "            # Extract article text\n",
    "            article_text = \"\"\n",
    "            paragraphs = soup.find_all('p')\n",
    "            #print(paragraphs)\n",
    "            if paragraphs: \n",
    "                for paragraph in paragraphs:\n",
    "                    article_text += paragraph.text.strip() + '\\n'\n",
    "                    #print(article_text)\n",
    "           \n",
    "            else:\n",
    "                article_div = soup.find('div', class_=lambda x: x and 'article' in x.lower())\n",
    "                if article_div:\n",
    "                    paragraphs = article_div.find_all('p')\n",
    "                    for paragraph in paragraphs:\n",
    "                        article_text += paragraph.text.strip() + '\\n'\n",
    "                else:\n",
    "                    print(\"Failed to find article text for URL:\", link)\n",
    "                    continue\n",
    "\n",
    "            # Write URL and article text to CSV file\n",
    "            writer.writerow([link, article_text])\n",
    "\n",
    "    print(\"Article texts extracted and saved to:\", output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "213f2e7b-3928-4c2c-8353-3901cebf397b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soup\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "loop\n",
      "Failed to fetch the webpage: https://ca.news.yahoo.com/inside-ice-cream-van-feeding-011756266.html\n",
      "Failed to fetch the webpage: https://www.yahoo.com/news/inside-ice-cream-van-feeding-011756266.html\n",
      "Failed to find article text for URL: https://www.msn.com/en-sg/news/world/households-warned-over-ramadan-burglaries/ar-BB1k4AUa\n",
      "Failed to fetch the webpage: https://ca.news.yahoo.com/ramadan-greater-manchester-households-warned-064603911.html\n",
      "Failed to fetch the webpage: https://www.yahoo.com/news/ramadan-greater-manchester-households-warned-064603911.html\n",
      "Failed to fetch the webpage: https://www.aol.com/news/ramadan-greater-manchester-households-warned-064603479.html\n",
      "Failed to fetch the webpage: https://www.yahoo.com/news/lancashire-homeowners-demand-action-over-170403282.html\n",
      "Failed to fetch the webpage: https://ca.news.yahoo.com/lancashire-homeowners-demand-action-over-170403282.html\n",
      "Failed to fetch the webpage: https://www.aol.com/news/2-000-super-strength-vapes-120740013.html\n",
      "Failed to fetch the webpage: https://www.aol.com/news/mum-surprises-twins-school-reunion-103832731.html\n",
      "Article texts extracted and saved to: Abbie_Jones.csv\n"
     ]
    }
   ],
   "source": [
    "Creative_url = 'https://muckrack.com/Abbie-Jones/articles'\n",
    "output_file = 'Abbie_Jones.csv'\n",
    "scrape_article_texts(Creative_url, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e68187-2d7f-4273-9525-335403cba09f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# IMPLEMENTATION FOR Iain Lynn THROUGH MUCKRACK.COM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28d696d7-9662-4d71-b3db-561458856f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_article_texts(author_url, output_file):\n",
    "    # Fetch HTML content\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.4'}\n",
    "    response = requests.get(author_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to fetch the webpage\")\n",
    "        return\n",
    "\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    print(\"soup\")\n",
    "    # Extract links\n",
    "    links = []\n",
    "    news = soup.find_all('h4', class_='news-story-title')\n",
    "    for new in news: \n",
    "        links.append(new.a['href'])\n",
    "        #print(\"loop\")\n",
    "        \n",
    "    # Scrape article text from each link\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['URL', 'Article Text'])\n",
    "\n",
    "        for link in links:\n",
    "            # Fetch HTML content of the article page\n",
    "            response = requests.get(link, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch the webpage: {link}\")\n",
    "                continue\n",
    "\n",
    "            # Parse HTML content\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            #print(soup)\n",
    "            # Extract article text\n",
    "            article_text = \"\"\n",
    "            paragraphs = soup.find_all('p')\n",
    "            #print(paragraphs)\n",
    "            if paragraphs: \n",
    "                for paragraph in paragraphs:\n",
    "                    article_text += paragraph.text.strip() + '\\n'\n",
    "                    #print(article_text)\n",
    "           \n",
    "            else:\n",
    "                article_div = soup.find('div', class_=lambda x: x and 'article' in x.lower())\n",
    "                if article_div:\n",
    "                    paragraphs = article_div.find_all('p')\n",
    "                    for paragraph in paragraphs:\n",
    "                        article_text += paragraph.text.strip() + '\\n'\n",
    "                else:\n",
    "                    print(\"Failed to find article text for URL:\", link)\n",
    "                    continue\n",
    "\n",
    "            # Write URL and article text to CSV file\n",
    "            writer.writerow([link, article_text])\n",
    "\n",
    "    print(\"Article texts extracted and saved to:\", output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f8bd9d7-4363-49ff-bc15-ef0880e3557e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soup\n",
      "Article texts extracted and saved to: Ian_Lynn.csv\n"
     ]
    }
   ],
   "source": [
    "Creative_url = 'https://muckrack.com/Iain-Lynn/articles'\n",
    "output_file = 'Ian_Lynn.csv'\n",
    "scrape_article_texts(Creative_url, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
